{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FairCoder Environment Setup for Google Colab\n",
    "\n",
    "This notebook automates the setup process for the FairCoder project in Google Colab. It will:\n",
    "1. Check the Python and CUDA environment\n",
    "2. Install required packages\n",
    "3. Set up Hugging Face Transformers\n",
    "4. Clone the FairCoder repository\n",
    "5. Set up OpenAI API (optional)\n",
    "\n",
    "**Note:** Google Colab environments are ephemeral. You'll need to run this setup each time you start a new session.\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI-Plans/evals/blob/main/setup_environment_colab.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Check\n",
    "\n",
    "Let's first check the Python version and GPU availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]\n",
      "Not running in Google Colab\n",
      "This notebook is designed for Google Colab environment.\n",
      "Thu Apr 17 09:48:37 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 572.70                 Driver Version: 572.70         CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4080 ...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   50C    P5             11W /   60W |    1974MiB /  12282MiB |     30%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A            2380    C+G   C:\\Windows\\System32\\dwm.exe           N/A      |\n",
      "|    0   N/A  N/A           13932    C+G   C:\\Windows\\explorer.exe               N/A      |\n",
      "|    0   N/A  N/A           19536    C+G   ...Center\\MSI.TerminalServer.exe      N/A      |\n",
      "|    0   N/A  N/A           28132    C+G   ...__kzh8wxbdkxb8p\\DCv2\\DCv2.exe      N/A      |\n",
      "|    0   N/A  N/A           31580    C+G   ...can\\IPSMONITOR\\iPSMonitor.exe      N/A      |\n",
      "|    0   N/A  N/A           31624    C+G   ...4__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A           43516    C+G   ...\\Sound Tune\\pro\\SoundTune.exe      N/A      |\n",
      "|    0   N/A  N/A           57204    C+G   ...Files\\Notepad++\\notepad++.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n",
      "NVIDIA GPU detected!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Print Python version\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if we're running in Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Not running in Google Colab\")\n",
    "    \n",
    "if not IN_COLAB:\n",
    "    print(\"This notebook is designed for Google Colab environment.\")\n",
    "\n",
    "# Check GPU availability\n",
    "try:\n",
    "    !nvidia-smi\n",
    "    HAS_GPU = True\n",
    "    print(\"\\nNVIDIA GPU detected!\")\n",
    "except:\n",
    "    HAS_GPU = False\n",
    "    print(\"\\nNo NVIDIA GPU detected. Using CPU only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive (Optional)\n",
    "\n",
    "If you want to save your work across sessions, it's recommended to mount your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment and run this cell if you want to mount your Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# PROJECT_DIR = '/content/drive/MyDrive/FairCoder'  # Change this path if needed\n",
    "# !mkdir -p {PROJECT_DIR}\n",
    "# %cd {PROJECT_DIR}\n",
    "\n",
    "# If not using Google Drive, use the Colab filesystem\n",
    "PROJECT_DIR = '/content/FairCoder'\n",
    "!mkdir -p {PROJECT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Required Packages\n",
    "\n",
    "Let's install PyTorch, required Python packages, and Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "PyTorch not found. Installing...\n",
      "PyTorch installed: 2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install openai numpy pandas -q\n",
    "\n",
    "# Check PyTorch installation\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(\"\\nTesting GPU tensor:\")\n",
    "        print(torch.rand(2, 3).cuda())\n",
    "    else:\n",
    "        print(\"\\nTesting CPU tensor:\")\n",
    "        print(torch.rand(2, 3))\n",
    "except ImportError:\n",
    "    print(\"PyTorch not found. Installing...\")\n",
    "    !pip install torch torchvision torchaudio -q\n",
    "    import torch\n",
    "    print(f\"PyTorch installed: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set up Hugging Face Transformers\n",
    "\n",
    "Let's install and configure Hugging Face Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers and huggingface_hub\n",
    "!pip install transformers huggingface_hub -q\n",
    "\n",
    "# Verify installation\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Hugging Face Login (Optional)\n",
    "\n",
    "To access gated models, you'll need to log in to Hugging Face. Create an account at [huggingface.co](https://huggingface.co/) and generate a token if you don't already have one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to log in to Hugging Face\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clone the FairCoder Repository\n",
    "\n",
    "Let's clone the FairCoder repository from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change to project directory\n",
    "%cd {PROJECT_DIR}\n",
    "\n",
    "# Check if repository already exists\n",
    "if os.path.exists(os.path.join(PROJECT_DIR, 'FairCoder')):\n",
    "    print(\"FairCoder repository already exists. Pulling latest changes...\")\n",
    "    %cd FairCoder\n",
    "    !git pull\n",
    "else:\n",
    "    print(\"Cloning FairCoder repository...\")\n",
    "    # Clone from AI-Plans fork\n",
    "    !git clone https://github.com/AI-Plans/FairCoder.git\n",
    "    %cd FairCoder\n",
    "\n",
    "# Verify repository\n",
    "!git status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set up OpenAI API (Optional)\n",
    "\n",
    "If you want to use OpenAI models, you'll need to set up the OpenAI API key. You can get an API key at [platform.openai.com](https://platform.openai.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to set up OpenAI API\n",
    "# import openai\n",
    "# from getpass import getpass\n",
    "\n",
    "# # Securely input API key (will not be visible)\n",
    "# openai_api_key = getpass(\"Enter your OpenAI API key: \")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "# openai.api_key = openai_api_key\n",
    "\n",
    "# # Verify OpenAI API key (minimal test)\n",
    "# try:\n",
    "#     models = openai.models.list()\n",
    "#     print(\"OpenAI API key is valid!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error with OpenAI API key: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Access to Gated Models (Optional)\n",
    "\n",
    "Let's test if we have access to the gated models mentioned in the instructions. You'll need to request access to these models at their respective Hugging Face pages:\n",
    "\n",
    "- [Meta-Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)\n",
    "- [Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)\n",
    "- [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
    "- [CodeGemma-7b-it](https://huggingface.co/google/codegemma-7b-it)\n",
    "- [unsloth/llama-3-8b-Instruct](https://huggingface.co/unsloth/llama-3-8b-Instruct) (alternative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to test access to gated models\n",
    "# from transformers import AutoTokenizer\n",
    "# import time\n",
    "\n",
    "# model_list = [\n",
    "#     \"meta-llama/Llama-2-7b-hf\",\n",
    "#     \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "#     \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "#     \"google/codegemma-7b-it\",\n",
    "#     \"unsloth/llama-3-8b-Instruct\"\n",
    "# ]\n",
    "\n",
    "# for model_name in model_list:\n",
    "#     print(f\"Testing access to {model_name}...\")\n",
    "#     try:\n",
    "#         # Just try to load the tokenizer, which is enough to test access\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         print(f\"✅ Access granted to {model_name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Cannot access {model_name}: {str(e)}\")\n",
    "#     time.sleep(1)  # Small delay to avoid rate limiting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Complete! 🎉\n",
    "\n",
    "Your FairCoder environment is now ready in Google Colab. Here's a summary of what we've done:\n",
    "\n",
    "1. ✅ Verified Python and CUDA environment\n",
    "2. ✅ Installed required packages: PyTorch, OpenAI, NumPy, Pandas\n",
    "3. ✅ Set up Hugging Face Transformers\n",
    "4. ✅ Cloned the FairCoder repository\n",
    "\n",
    "Optional steps you can complete:\n",
    "1. Mount Google Drive for persistent storage\n",
    "2. Log in to Hugging Face to access gated models\n",
    "3. Set up OpenAI API for using OpenAI models\n",
    "4. Test access to gated models\n",
    "\n",
    "**Reminder:** Since Google Colab sessions are temporary, you'll need to run this setup again when you start a new session. You might want to save this notebook to your Google Drive for easy access."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
